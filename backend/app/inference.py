import os
import io
import tempfile
import math
from typing import Dict, Any, Tuple, List

import requests
import time, hashlib, logging
from PIL import Image, ImageOps
from google.cloud import storage
from ultralytics import YOLO

import gc
import torch
from app.common import polygon_centroid

# ===============================================
# ğŸ”‘ Ø«ÙˆØ§Ø¨Øª Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„Ù†Ù…Ø°Ø¬Ø© (Detection/Inference)
# ===============================================
TILE_SIZE = 1024        # Ø­Ø¬Ù… Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù…Ù† Ø§Ù„ØªØ¨Ù„ÙŠØ· (1024x1024)
OVERLAP = 0.20          # Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ¯Ø§Ø®Ù„ (0.20)
CONF_THRESHOLD = 0.30   # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ Ù„Ø«Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ (Ø§Ù„Ø«Ù‚Ø©)
NMS_IOU_THRESHOLD = 0.70  # Ù‚ÙŠÙ…Ø© IoU Ù„Ù€ NMS
MAX_DETECTION_LIMIT = 5000  # Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø§ÙƒØªØ´Ø§ÙØ§Øª Ù„Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø°Ø§ÙƒØ±Ø©

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª MapTiler
MAPTILER_KEY = os.environ.get("MAPTILER_KEY")
TILE_URL = "https://api.maptiler.com/maps/satellite/{zoom}/{x}/{y}.jpg?key={key}"
TILE_SIZE_MAP = 512     # Ø­Ø¬Ù… Ø§Ù„ØªØ¨Ù„ÙŠØ· Ø§Ù„ÙØ¹Ù„ÙŠ Ù„Ù€ MapTiler (512x512)

# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª GCS
DEFAULT_BUCKET = os.environ.get("STORAGE_BUCKET", "saaf-97251.firebasestorage.app")
MODELS_PREFIX = os.environ.get("REMOTE_MODELS_PREFIX", "models/")
MODELS_GCS_URI_A = os.environ.get("MODELS_GCS_URI_A")
MODELS_GCS_URI_B = os.environ.get("MODELS_GCS_URI_B")


# -------- Ø£Ø¯ÙˆØ§Øª GCS --------

def _gcs() -> storage.Client:
    return storage.Client()


def _download_blob(bucket_name: str, blob_name: str) -> str:
    client = _gcs()
    bucket = client.bucket(bucket_name)
    blob = bucket.blob(blob_name)
    if not blob.exists():
        raise FileNotFoundError(f"Blob not found: gs://{bucket_name}/{blob_name}")
    fd, tmp = tempfile.mkstemp(suffix=".pt")
    os.close(fd)
    blob.download_to_filename(tmp)
    print(f"âœ… Downloaded gs://{bucket_name}/{blob_name} -> {tmp}")
    return tmp


def _parse_gs_uri(uri: str) -> Tuple[str, str]:
    without = uri.replace("gs://", "")
    bucket = without.split("/")[0]
    blob = "/".join(without.split("/")[1:])
    return bucket, blob


def _auto_pick_two_pt(bucket: str, prefix: str) -> Tuple[str, str]:
    client = _gcs()
    blobs = list(client.list_blobs(bucket, prefix=prefix))
    pts = [b for b in blobs if b.name.lower().endswith(".pt")]
    if len(pts) < 2:
        raise FileNotFoundError(f"Need >=2 .pt under gs://{bucket}/{prefix}")

    finetuned = [b for b in pts if os.path.basename(b.name).startswith("best_finetuned")]
    best_named = [b for b in pts if os.path.basename(b.name) == "best.pt"]

    if finetuned and best_named:
        fa = sorted(finetuned, key=lambda b: b.updated or b.time_created, reverse=True)[0]
        bb = sorted(best_named, key=lambda b: b.updated or b.time_created, reverse=True)[0]
        return fa.name, bb.name

    pts_sorted = sorted(pts, key=lambda b: b.updated or b.time_created, reverse=True)
    return pts_sorted[0].name, pts_sorted[1].name


# -------- ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† --------
def load_models_auto():
    if MODELS_GCS_URI_A and MODELS_GCS_URI_B:
        a_bucket, a_blob = _parse_gs_uri(MODELS_GCS_URI_A)
        b_bucket, b_blob = _parse_gs_uri(MODELS_GCS_URI_B)

        a_local = _download_blob(a_bucket, a_blob)
        b_local = _download_blob(b_bucket, b_blob)

        model_a = YOLO(a_local)
        model_a.model.to("cpu").eval()
        model_b = YOLO(b_local)
        model_b.model.to("cpu").eval()

        return {"A": model_a, "B": model_b}, {
            "A": MODELS_GCS_URI_A,
            "B": MODELS_GCS_URI_B,
        }

    blob_a, blob_b = _auto_pick_two_pt(DEFAULT_BUCKET, MODELS_PREFIX)

    a_local = _download_blob(DEFAULT_BUCKET, blob_a)
    b_local = _download_blob(DEFAULT_BUCKET, blob_b)

    model_a = YOLO(a_local)
    model_a.model.to("cpu").eval()
    model_b = YOLO(b_local)
    model_b.model.to("cpu").eval()

    return (
        {"A": model_a, "B": model_b},
        {
            "A": f"gs://{DEFAULT_BUCKET}/{blob_a}",
            "B": f"gs://{DEFAULT_BUCKET}/{blob_b}",
        },
    )


# ===== helpers: retry + safe-open + logging =====
def _http_get_with_retry(url: str, tries: int = 3, backoff: float = 0.75, timeout: int = 30):
    last = None
    for i in range(tries):
        r = requests.get(url, timeout=timeout)
        logging.info(
            f"[FETCH] url={url} status={r.status_code} bytes={len(r.content)} try={i + 1}/{tries}"
        )
        if r.status_code < 500:
            r.raise_for_status()
            return r
        last = r
        time.sleep(backoff * (2 ** i))
    last.raise_for_status()


def _open_fix_to_rgb(raw_bytes: bytes, tag: str):
    """ØªØµØ­ÙŠØ­ EXIFØŒ Ø¥Ø²Ø§Ù„Ø© alphaØŒ ÙˆØ§Ù„ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ RGB."""
    im = Image.open(io.BytesIO(raw_bytes))
    try:
        im = ImageOps.exif_transpose(im)
    except Exception as e:
        logging.info(f"[IMG] exif_transpose_fail tag={tag} err={e}")

    if im.mode in ("RGBA", "LA"):
        bg = Image.new("RGB", im.size, (0, 0, 0))
        im_alpha = im.split()[-1]
        bg.paste(im, mask=im_alpha)
        im = bg
    elif im.mode != "RGB":
        im = im.convert("RGB")

    logging.info(f"[IMG] opened tag={tag} mode={im.mode} size={im.size}")
    return im


# -------- Ø£Ø¯ÙˆØ§Øª MapTiler (Ø§Ù„ØªØ¨Ù„ÙŠØ·) --------

def _deg_to_tile(lat: float, lon: float, zoom: int) -> Tuple[int, int]:
    lat_rad = math.radians(lat)
    n = 2.0 ** zoom
    xtile = int((lon + 180.0) / 360.0 * n)
    ytile = int(
        (
            n
            * (1.0 - math.log(math.tan(lat_rad) + (1 / math.cos(lat_rad))) / math.pi)
            / 2.0
        )
    )
    return xtile, ytile


def _download_and_stitch_tiles(
    lat: float, lon: float, zoom: int, target_size: int = TILE_SIZE
) -> Image.Image:
    if not MAPTILER_KEY:
        raise RuntimeError("MAPTILER_KEY is required")

    cx, cy = _deg_to_tile(lat, lon, zoom)

    # Ù†Ø³ØªØ®Ø¯Ù… TILE_SIZE_MAP (512) Ù„Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª
    tiles_per_side = target_size // TILE_SIZE_MAP
    if tiles_per_side < 1:
        tiles_per_side = 1

    start_x = cx - (tiles_per_side // 2)
    start_y = cy - (tiles_per_side // 2)
    stitched_image = Image.new(
        "RGB", (tiles_per_side * TILE_SIZE_MAP, tiles_per_side * TILE_SIZE_MAP)
    )

    for i in range(tiles_per_side):
        for j in range(tiles_per_side):
            x = start_x + i
            y = start_y + j
            url = TILE_URL.format(zoom=zoom, x=x, y=y, key=MAPTILER_KEY)

            r = _http_get_with_retry(url, tries=3, backoff=0.75, timeout=60)
            tile_image = Image.open(io.BytesIO(r.content)).convert("RGB")

            # Ù†Ø³ØªØ®Ø¯Ù… TILE_SIZE_MAP Ù„Ù„ØµÙ‚
            stitched_image.paste(tile_image, (i * TILE_SIZE_MAP, j * TILE_SIZE_MAP))

    return stitched_image.crop((0, 0, target_size, target_size))


def get_sat_image_for_farm(farm: Dict[str, Any]) -> str:
    """
    ÙŠØ­Ø¯Ø¯ Ù…Ø±ÙƒØ² Ø§Ù„Ù…Ø¶Ù„Ø¹ ÙˆÙŠÙ†Ø²Ù‘Ù„ ØµÙˆØ±Ø© Ø§Ù„Ø£Ù‚Ù…Ø§Ø± Ø§Ù„ØµÙ†Ø§Ø¹ÙŠØ© (Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Tiles) Ø«Ù… ÙŠØ­ÙØ¸Ù‡Ø§.
    """
    img_url = (farm.get("imageURL") or farm.get("imageUrl") or "").strip()

    # 1) ØµÙˆØ±Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… (Ø¥Ù† ÙˆØ¬Ø¯Øª): retry + EXIF + RGB + resize => TILE_SIZE
    if img_url:
        r = _http_get_with_retry(img_url, tries=2, backoff=0.75, timeout=120)
        sha1 = hashlib.sha1(r.content).hexdigest()
        logging.info(
            f"[SRC] user content_type={r.headers.get('Content-Type')} "
            f"bytes={len(r.content)} sha1={sha1}"
        )
        img = _open_fix_to_rgb(r.content, tag="user")
        img = img.resize((TILE_SIZE, TILE_SIZE))
        path = "/tmp/input.jpg"
        img.save(path, "JPEG", quality=90)
        logging.info(f"[IMG] saved path={path} source=user")
        return path

    # 2) Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø±Ø§Ø¨Ø· URL Ù…ØªØ§Ø­Ø§Ù‹ØŒ Ø§Ø³ØªØ®Ø¯Ù… MapTiler Tiles
    poly = farm.get("polygon") or []
    if not poly or len(poly) < 3:
        raise ValueError("Farm polygon is missing or < 3 points")

    lat, lon = polygon_centroid(poly)

    # ØªØ­Ù…ÙŠÙ„ ÙˆØ¯Ù…Ø¬ Ø§Ù„Ù…Ø±Ø¨Ø¹Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TILE_SIZE (1024)
    img = _download_and_stitch_tiles(lat=lat, lon=lon, zoom=18, target_size=TILE_SIZE)

    logging.info(f"[SRC] maptiler size={img.size}")
    path = "/tmp/input.jpg"
    img.convert("RGB").save(path, "JPEG", quality=85)
    return path


# -------- Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ --------

def _yolo_predict(model: YOLO, image_path: str) -> Dict[str, Any]:
    try:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù‚Ø¨Ù„ Ø§Ù„Ø¨Ø¯Ø¡
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        results = model.predict(
            image_path,
            device="cpu",
            verbose=False,
            conf=CONF_THRESHOLD,
            iou=NMS_IOU_THRESHOLD,
            max_det=MAX_DETECTION_LIMIT,
        )

        r = results[0]
        dets: List[Dict[str, Any]] = []

        if hasattr(r, "boxes") and r.boxes is not None:
            names = getattr(r, "names", {}) or {}
            limit = min(len(r.boxes), MAX_DETECTION_LIMIT)

            for i in range(limit):
                b = r.boxes[i]
                cls = int(b.cls[0])
                conf = float(b.conf[0])

                if conf >= CONF_THRESHOLD:
                    x1, y1, x2, y2 = [float(v) for v in b.xyxy[0].tolist()]
                    dets.append(
                        {
                            "cls": cls,
                            "label": names.get(cls, str(cls)),
                            "conf": conf,
                            "box_xyxy": [x1, y1, x2, y2],
                        }
                    )

        mean_conf = (sum(d["conf"] for d in dets) / len(dets)) if dets else 0.0
        score = float(mean_conf + 0.05 * math.log(1 + len(dets)))
        return {"detections": dets, "count": len(dets), "score": score}

    finally:
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()


def run_both_and_pick_best(models, image_path: str) -> Dict[str, Any]:
    """
    ØªØ´ØºÙ‘Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† A Ùˆ B ÙˆØªØ¹ÙŠØ¯ ÙƒÙ„ Ø§Ù„ØªÙØ§ØµÙŠÙ„ (ØªÙˆØ§ÙÙ‚Ø§Ù‹ Ù…Ø¹ Ø§Ù„ÙƒÙˆØ¯ Ø§Ù„Ù‚Ø¯ÙŠÙ…).
    """
    a = _yolo_predict(models["A"], image_path)
    b = _yolo_predict(models["B"], image_path)
    best = a if a["score"] >= b["score"] else b
    return {
        "picked": "A" if best is a else "B",
        "count": best["count"],
        "score": best["score"],
        "best_detections": best["detections"],
        "A": {"count": a["count"], "score": a["score"]},
        "B": {"count": b["count"], "score": b["score"]},
    }


# ===============================================
# ğŸ§® Ø¯Ø§Ù„Ø© Count Ù…Ø¨Ø³Ù‘Ø·Ø© Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ù…Ø¹ Ø§Ù„Ø¯Ø§ØªØ§Ø¨ÙŠØ³
# ===============================================

def count_palms(models, image_path: str) -> Dict[str, Any]:
    """
    Ø¯Ø§Ù„Ø© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ù…Ø³ØªÙˆÙ‰ ØªÙ‚ÙˆÙ… Ø¨Ø§Ù„Ø¹Ø¯:
    - ØªØ´ØºÙ‘Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ÙŠÙ† A Ùˆ B
    - ØªØ®ØªØ§Ø± Ø§Ù„Ø£ÙØ¶Ù„ Ø¯Ø§Ø®Ù„ÙŠØ§Ù‹
    - ØªØ±Ø¬Ø¹ ÙÙ‚Ø· Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù„ÙŠ Ù†Ø­ØªØ§Ø¬Ù‡Ø§ Ù„Ù„ØªØ®Ø²ÙŠÙ† ÙÙŠ Ø§Ù„Ø¯Ø§ØªØ§Ø¨ÙŠØ³ Ø£Ùˆ Ø§Ù„ØªÙ‚Ø±ÙŠØ±

    Ø§Ù„Ø´ÙƒÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ:
    {
        "count": <int>,        # Ø¹Ø¯Ø¯ Ø§Ù„Ù†Ø®ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        "quality": <float>,    # Ø¬ÙˆØ¯Ø© Ø§Ù„Ø¹Ø¯ (score)
        "model": "A" Ø£Ùˆ "B"    # Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ ØªÙ… Ø§Ø®ØªÙŠØ§Ø±Ù‡
    }
    """
    result = run_both_and_pick_best(models, image_path)

    return {
        "count": int(result["count"]),
        "quality": float(result["score"]),
        "model": result.get("picked"),
    }
